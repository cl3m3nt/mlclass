{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python361064bit44e439ebc2cc4be1b2702145ac5a3ea8",
   "display_name": "Python 3.6.10 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self,input_len,idx):\n",
    "        self.bias = 1 # init bias to 1\n",
    "        self.w = np.random.rand(input_len) # random value weight list, input_len is the dimension of the x_input vector\n",
    "        self.output = 0 # init output to 0\n",
    "        self.index = idx\n",
    "\n",
    "    def printIndex(self):\n",
    "        print(\"Neuron: \",self.index)\n",
    "\n",
    "    def linearActivation(self,x):\n",
    "        self.output = self.bias\n",
    "        for i in range(0,len(x)):\n",
    "            self.output = self.output + self.w[i] * x[i]\n",
    "        return self.output\n",
    "\n",
    "    def reluActivation(self,x):\n",
    "        self.output = self.bias\n",
    "        for i in range(0,len(x)):\n",
    "            self.output = self.output + self.w[i] * x[i]\n",
    "        if(self.output > 0):\n",
    "            self.output = self.output\n",
    "        else:\n",
    "            self.output = 0\n",
    "        return self.output\n",
    "\n",
    "    def tanhActivation(self,x):\n",
    "        self.output = self.bias\n",
    "        for i in range(0,len(x)):\n",
    "            self.output = self.output + self.w[i] * x[i]\n",
    "        self.output = np.tanh(self.output)\n",
    "        return self.output\n",
    "\n",
    "    def sigmoidActivation(self,x):\n",
    "        self.output = self.bias\n",
    "        for i in range(0,len(x)):\n",
    "            self.output = self.output + self.w[i] * x[i]\n",
    "        self.output = 1 / (1+np.exp(-self.output))\n",
    "        return self.output\n",
    "\n",
    "    def softmaxActivation(self,x,k):\n",
    "        self.output = self.bias\n",
    "        for i in range(0,len(x)):\n",
    "            self.output = self.output + np.exp(self.w[i] * x[i])\n",
    "        self.output = np.exp(x[k]) / self.output\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self,lay_size):\n",
    "        self.layer_size = lay_size\n",
    "        self.neurons = []\n",
    "        self.output_size = lay_size\n",
    "        self.outputs = []\n",
    "    \n",
    "    def createNeurons(self,input_size):\n",
    "        for i in range(0,self.layer_size):\n",
    "            self.neurons.append(Neuron(input_size,i))\n",
    "        return self.neurons\n",
    "\n",
    "    def linearOutputs(self,x):\n",
    "        self.outputs = []\n",
    "        for i in range(0,self.layer_size):\n",
    "            self.outputs.append(self.neurons[i].linearActivation(x))\n",
    "        return self.outputs\n",
    "\n",
    "    def reluOutputs(self,x):\n",
    "        self.outputs = []\n",
    "        for i in range(0,self.layer_size):\n",
    "            self.outputs.append(self.neurons[i].reluActivation(x))\n",
    "        return self.outputs\n",
    "    \n",
    "    def tanhOutputs(self,x):\n",
    "        self.outputs = []\n",
    "        for i in range(0,self.layer_size):\n",
    "            self.outputs.append(self.neurons[i].tanhActivation(x))\n",
    "        return self.outputs\n",
    "    \n",
    "    def sigmoidOutputs(self,x):\n",
    "        self.outputs = []\n",
    "        for i in range(0,self.layer_size):\n",
    "            self.outputs.append(self.neurons[i].sigmoidActivation(x))\n",
    "        return self.outputs\n",
    "\n",
    "    def softmaxOutputs(self,x,k):\n",
    "        self.outputs = []\n",
    "        for i in range(0,self.layer_size):\n",
    "            self.outputs.append(self.neurons[i].softmaxActivation(x,k))\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Layers of different sizes\n",
    "x = np.array([1,1,1,1])\n",
    "layer1_size = 8\n",
    "layer2_size = 16\n",
    "layer3_size = 32\n",
    "'''Layer1 with 8 Neurons'''\n",
    "myLayer1 = Layer(layer1_size)\n",
    "'''Layer2 with 16 Neurons'''\n",
    "myLayer2 = Layer(layer2_size)\n",
    "'''Layer3 with 32 Neurons'''\n",
    "myLayer3 = Layer(layer3_size)\n",
    "\n",
    "# Testing Neurons creation for layers of different size\n",
    "print(\"Layer 1\")\n",
    "myLayer1Neurons = myLayer1.createNeurons(len(x))\n",
    "for i in range(0,layer1_size):\n",
    "    myLayer1Neurons[i].printIndex()\n",
    "    print(myLayer1Neurons[i].w)\n",
    "    print(np.sum(myLayer1Neurons[i].w))\n",
    "\n",
    "'''print(\"\\n\")\n",
    "print(\"Layer 2\")\n",
    "myLayer2Neurons = myLayer2.createNeurons(len(x))\n",
    "for i in range(0,layer2_size):\n",
    "    myLayer2Neurons[i].printIndex()\n",
    "    print(myLayer2Neurons[i].w)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Layer 3\")\n",
    "myLayer3Neurons = myLayer3.createNeurons(len(x))\n",
    "for i in range(0,layer3_size):\n",
    "    myLayer3Neurons[i].printIndex()\n",
    "    print(myLayer3Neurons[i].w)'''\n",
    "\n",
    "# Testing Layer outputs calculation for layer of different size\n",
    "print(\"\\n\")\n",
    "print(\"Layer Outputs\")\n",
    "print(\"linear output:\",myLayer1.linearOutputs(x))\n",
    "print(\"relu output:\",myLayer1.reluOutputs(x))\n",
    "print(\"tanh output:\",myLayer1.tanhOutputs(x))\n",
    "print(\"sigmoid output:\",myLayer1.sigmoidOutputs(x))\n",
    "print(\"softmax output:\",myLayer1.softmaxOutputs(x,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Layer 1\nNeuron:  0\n[0.95288004 0.2392245  0.9064938  0.83250147]\n2.9310998080789568\nNeuron:  1\n[0.05476278 0.3968844  0.02756982 0.92304877]\n1.4022657700573224\nNeuron:  2\n[0.87592399 0.41166997 0.32372091 0.73184343]\n2.343158307862258\nNeuron:  3\n[0.72492788 0.4250675  0.87879167 0.4946998 ]\n2.5234868434325\nNeuron:  4\n[0.80134156 0.14781022 0.88056413 0.16605039]\n1.9957663164174688\nNeuron:  5\n[0.67826509 0.82298979 0.14144108 0.60425722]\n2.2469531737673227\nNeuron:  6\n[0.02156006 0.51839881 0.27364721 0.84389942]\n1.6575055000226742\nNeuron:  7\n[0.52111468 0.27781027 0.87063318 0.67186156]\n2.3414196914776486\n\n\nLayer 2\nNeuron:  0\n[0.24790059 0.6791941  0.41787846 0.97275499 0.06306455 0.8529253\n 0.15729149 0.18597353]\nNeuron:  1\n[0.17849497 0.04252327 0.45474282 0.71243722 0.29849454 0.44471193\n 0.55327455 0.55830797]\nNeuron:  2\n[0.10145008 0.64557841 0.33884581 0.43871806 0.16555207 0.42000165\n 0.77513677 0.03584418]\nNeuron:  3\n[0.43790369 0.96823567 0.51534392 0.84033797 0.20128703 0.49017988\n 0.813666   0.8912651 ]\nNeuron:  4\n[0.36587578 0.38119708 0.02442245 0.68567433 0.03181056 0.33077297\n 0.43242661 0.67392048]\nNeuron:  5\n[0.0567698  0.34994666 0.43068861 0.95718806 0.9202696  0.1991185\n 0.35674513 0.02377897]\nNeuron:  6\n[0.1631615  0.90336076 0.75011203 0.95829212 0.13087774 0.47633525\n 0.69663243 0.93534463]\nNeuron:  7\n[0.42474262 0.11998811 0.54278173 0.43161676 0.5589158  0.82455924\n 0.94513722 0.06087003]\nNeuron:  8\n[0.39869364 0.74611252 0.75886566 0.84427774 0.43611769 0.86045672\n 0.94925271 0.97885857]\nNeuron:  9\n[0.16550569 0.21949838 0.49289413 0.37424472 0.77386415 0.82144294\n 0.32377158 0.78826065]\nNeuron:  10\n[0.34013937 0.0831597  0.01047602 0.66934028 0.80831204 0.08350276\n 0.274301   0.40955479]\nNeuron:  11\n[0.90948354 0.02400117 0.93366732 0.74145235 0.68787086 0.55680627\n 0.07113404 0.31072766]\nNeuron:  12\n[0.90582699 0.60694893 0.60106627 0.36261468 0.34214216 0.85499488\n 0.51281027 0.08819199]\nNeuron:  13\n[0.37420907 0.48751232 0.35963693 0.20460592 0.09272771 0.05301534\n 0.65689276 0.08661168]\nNeuron:  14\n[0.37874783 0.00518774 0.90668169 0.98055845 0.67466804 0.22346449\n 0.08166804 0.87780199]\nNeuron:  15\n[0.91936333 0.3116138  0.21421479 0.14339275 0.05118158 0.36563241\n 0.99408668 0.00567052]\n\n\nLayer Outputs\nlinear output: [3.317728142185996, 2.3881982783030065, 2.5245923592713226, 3.7618212573992915, 2.457169635895245, 2.794593134867991, 3.7749264131435, 2.5191292232506095, 3.7479495648235357, 2.252142923089708, 2.1031153809711456, 3.608604384443755, 3.476456870780255, 2.425964242929256, 3.2711757091632303, 2.5885846727334414]\n"
    }
   ],
   "source": [
    "# Testing Layers of different sizes\n",
    "x = np.array([1,1,1,1])\n",
    "layer1_size = 8\n",
    "layer2_size = 16\n",
    "layer3_size = 32\n",
    "'''Layer1 with 8 Neurons'''\n",
    "myLayer1 = Layer(layer1_size)\n",
    "'''Layer2 with 16 Neurons'''\n",
    "myLayer2 = Layer(layer2_size)\n",
    "\n",
    "# Testing Neurons creation for layers of different size\n",
    "print(\"Layer 1\")\n",
    "myLayer1Neurons = myLayer1.createNeurons(len(x))\n",
    "for i in range(0,layer1_size):\n",
    "    myLayer1Neurons[i].printIndex()\n",
    "    print(myLayer1Neurons[i].w)\n",
    "    print(np.sum(myLayer1Neurons[i].w))\n",
    "\n",
    "\n",
    "# Testing Layer outputs calculation for layer of different size\n",
    "l1_linearOutputs = myLayer1.linearOutputs(x)\n",
    "l1_reluOutputs= myLayer1.reluOutputs(x)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Layer 2\")\n",
    "myLayer2Neurons = myLayer2.createNeurons(layer1_size) # layer1_size is now used for input dimension of layer 2 Neurons\n",
    "for i in range(0,layer2_size):\n",
    "    myLayer2Neurons[i].printIndex()\n",
    "    print(myLayer2Neurons[i].w)\n",
    "\n",
    "# Testing Layer outputs length of layer 2\n",
    "print(\"\\n\")\n",
    "print(\"Layer Outputs\")\n",
    "print(\"linear output:\",myLayer2.linearOutputs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}