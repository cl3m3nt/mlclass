{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bitconda486fd7c9eaad4a85a07513325d6d1998",
   "display_name": "Python 3.6.8 64-bit (conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self,input_len,idx):\n",
    "        self.bias = 1 # init bias to 1\n",
    "        self.w = np.random.rand(input_len) # random value weight list, input_len is the dimension of the x_input vector\n",
    "        self.output = 0 # init output to 0\n",
    "        self.index = idx\n",
    "\n",
    "    def printIndex(self):\n",
    "        print(\"Neuron: \",self.index)\n",
    "\n",
    "    def linearActivation(self,x):\n",
    "        self.output = self.bias\n",
    "        for i in range(0,len(x)):\n",
    "            self.output = self.output + self.w[i] * x[i]\n",
    "        return self.output\n",
    "\n",
    "    def reluActivation(self,x):\n",
    "        self.output = self.bias\n",
    "        for i in range(0,len(x)):\n",
    "            self.output = self.output + self.w[i] * x[i]\n",
    "        if(self.output > 0):\n",
    "            self.output = self.output\n",
    "        else:\n",
    "            self.output = 0\n",
    "        return self.output\n",
    "\n",
    "    def tanhActivation(self,x):\n",
    "        self.output = self.bias\n",
    "        for i in range(0,len(x)):\n",
    "            self.output = self.output + self.w[i] * x[i]\n",
    "        self.output = np.tanh(self.output)\n",
    "        return self.output\n",
    "\n",
    "    def sigmoidActivation(self,x):\n",
    "        self.output = self.bias\n",
    "        for i in range(0,len(x)):\n",
    "            self.output = self.output + self.w[i] * x[i]\n",
    "        self.output = 1 / (1+np.exp(-self.output))\n",
    "        return self.output\n",
    "\n",
    "    def softmaxActivation(self,x,k):\n",
    "        self.output = self.bias\n",
    "        for i in range(0,len(x)):\n",
    "            self.output = self.output + np.exp(self.w[i] * x[i])\n",
    "        self.output = np.exp(x[k]) / self.output\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self,lay_size):\n",
    "        self.layer_size = lay_size\n",
    "        self.neurons = []\n",
    "        self.output_size = lay_size\n",
    "        self.outputs = []\n",
    "    \n",
    "    def createNeurons(self,input_size):\n",
    "        for i in range(0,self.layer_size):\n",
    "            self.neurons.append(Neuron(input_size,i))\n",
    "        return self.neurons\n",
    "\n",
    "    def linearOutputs(self,x):\n",
    "        self.outputs = []\n",
    "        for i in range(0,self.layer_size):\n",
    "            self.outputs.append(self.neurons[i].linearActivation(x))\n",
    "        return self.outputs\n",
    "\n",
    "    def reluOutputs(self,x):\n",
    "        self.outputs = []\n",
    "        for i in range(0,self.layer_size):\n",
    "            self.outputs.append(self.neurons[i].reluActivation(x))\n",
    "        return self.outputs\n",
    "    \n",
    "    def tanhOutputs(self,x):\n",
    "        self.outputs = []\n",
    "        for i in range(0,self.layer_size):\n",
    "            self.outputs.append(self.neurons[i].tanhActivation(x))\n",
    "        return self.outputs\n",
    "    \n",
    "    def sigmoidOutputs(self,x):\n",
    "        self.outputs = []\n",
    "        for i in range(0,self.layer_size):\n",
    "            self.outputs.append(self.neurons[i].sigmoidActivation(x))\n",
    "        return self.outputs\n",
    "\n",
    "    def softmaxOutputs(self,x,k):\n",
    "        self.outputs = []\n",
    "        for i in range(0,self.layer_size):\n",
    "            self.outputs.append(self.neurons[i].softmaxActivation(x,k))\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 x Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Layer\n",
    "x = np.array([1,1,1,1])\n",
    "layer1_size = 8\n",
    "\n",
    "'''Layer1 with 8 Neurons'''\n",
    "myLayer1 = Layer(layer1_size)\n",
    "\n",
    "# Testing Neurons creation for layer\n",
    "print(\"Layer 1\")\n",
    "myLayer1Neurons = myLayer1.createNeurons(len(x))\n",
    "for i in range(0,layer1_size):\n",
    "    myLayer1Neurons[i].printIndex()\n",
    "    print(myLayer1Neurons[i].w)\n",
    "    print(np.sum(myLayer1Neurons[i].w))\n",
    "\n",
    "# Testing Layer outputs calculation for layer of different size\n",
    "print(\"\\n\")\n",
    "print(\"Layer Outputs:\")\n",
    "print(\"linear output:\",myLayer1.linearOutputs(x))\n",
    "print(\"relu output:\",myLayer1.reluOutputs(x))\n",
    "print(\"tanh output:\",myLayer1.tanhOutputs(x))\n",
    "print(\"sigmoid output:\",myLayer1.sigmoidOutputs(x))\n",
    "print(\"softmax output:\",myLayer1.softmaxOutputs(x,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 x Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Layers of different sizes\n",
    "x = np.array([1,1,1,1])\n",
    "layer1_size = 8\n",
    "layer2_size = 16\n",
    "layer3_size = 32\n",
    "'''Layer1 with 8 Neurons'''\n",
    "myLayer1 = Layer(layer1_size)\n",
    "'''Layer2 with 16 Neurons'''\n",
    "myLayer2 = Layer(layer2_size)\n",
    "\n",
    "# Testing Neurons creation for layers of different size\n",
    "print(\"Layer 1\")\n",
    "myLayer1Neurons = myLayer1.createNeurons(len(x))\n",
    "for i in range(0,layer1_size):\n",
    "    myLayer1Neurons[i].printIndex()\n",
    "    print(myLayer1Neurons[i].w)\n",
    "    print(np.sum(myLayer1Neurons[i].w))\n",
    "\n",
    "\n",
    "# Testing Layer outputs calculation for layer of different size\n",
    "l1_linearOutputs = myLayer1.linearOutputs(x)\n",
    "l1_reluOutputs= myLayer1.reluOutputs(x)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Layer 2\")\n",
    "myLayer2Neurons = myLayer2.createNeurons(layer1_size) # layer1_size is now used for input dimension of layer 2 Neurons\n",
    "for i in range(0,layer2_size):\n",
    "    myLayer2Neurons[i].printIndex()\n",
    "    print(myLayer2Neurons[i].w)\n",
    "\n",
    "# Testing Layer outputs length of layer 2\n",
    "print(\"\\n\")\n",
    "print(\"Layer Outputs\")\n",
    "print(\"linear output:\",myLayer2.linearOutputs(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3 x Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init input vector x and layers nb of neurons\n",
    "x = np.array([1,1,1,1]) # input vector x of size 4\n",
    "first_layer_size = 8 \n",
    "second_layer_size = 16 \n",
    "third_layer_size = 8\n",
    "\n",
    "# create Layers L1,L2,L3\n",
    "layer1 = Layer(first_layer_size)\n",
    "layer2 = Layer(second_layer_size)\n",
    "layer3 = Layer(third_layer_size)\n",
    "\n",
    "# create Neurons for L1,L2,L3\n",
    "l1_neurons = layer1.createNeurons(len(x)) # 8 x Neurons with 4 x input values for each Neuron\n",
    "l2_neurons = layer2.createNeurons(layer1.output_size) # 16 x Neurons with 8 x input values for each Neuron\n",
    "l3_neurons = layer3.createNeurons(layer2.output_size) # 8 x Neurons with 16 x input values for each Neuron\n",
    "\n",
    "# compute outputs of each Neurons for L1,L2,L3\n",
    "l1_outputs = layer1.linearOutputs(x)\n",
    "l2_outputs = layer2.linearOutputs(l1_outputs)\n",
    "l3_outputs = layer3.linearOutputs(l2_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute output of each layer when sending in x = [1,1,1,1]\n",
    "print(\"layer1 outputs:\",layer1.linearOutputs(x))\n",
    "print(\"\\n\")\n",
    "print(\"layer2 outputs:\",layer2.linearOutputs(layer1.outputs))\n",
    "print(\"\\n\")\n",
    "print(\"layer3 outputs:\",layer3.linearOutputs(layer2.outputs))\n",
    "# obervation: output of layer 1 is of size 8, output of layer 2 is of size 16, output of layer 3 is 8\n",
    "# observation: input vector x=[1,1,1,1] <-> v(4), becomes then v(8) out of L1, becomes then v(16) out of L2, becomes then v(8) out of L3\n",
    "# observation: output vector v of any Layer L of size k has a length k giving v(k)\n",
    "# observation: input vector x=[1,1,1,1] ends to be a vector v(8) = [a,b,c,d,e,f,g,h] out of L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.outputs = []\n",
    "    \n",
    "    def addLayers(self,layer):\n",
    "        self.layers.append(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNeuralNetwork = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNeuralNetwork.addLayers(layer1)\n",
    "myNeuralNetwork.addLayers(layer2)\n",
    "myNeuralNetwork.addLayers(layer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<__main__.Layer object at 0x111fb9128>\nNeuron:  0\nNeuron:  1\nNeuron:  2\nNeuron:  3\nNeuron:  4\nNeuron:  5\nNeuron:  6\nNeuron:  7\n<__main__.Layer object at 0x111fb90f0>\nNeuron:  0\nNeuron:  1\nNeuron:  2\nNeuron:  3\nNeuron:  4\nNeuron:  5\nNeuron:  6\nNeuron:  7\nNeuron:  8\nNeuron:  9\nNeuron:  10\nNeuron:  11\nNeuron:  12\nNeuron:  13\nNeuron:  14\nNeuron:  15\n<__main__.Layer object at 0x111fb9198>\nNeuron:  0\nNeuron:  1\nNeuron:  2\nNeuron:  3\nNeuron:  4\nNeuron:  5\nNeuron:  6\nNeuron:  7\n"
    }
   ],
   "source": [
    "for layer in myNeuralNetwork.layers:\n",
    "    print(layer)\n",
    "    for neuron in layer.neurons:\n",
    "        neuron.printIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}